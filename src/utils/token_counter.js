/**
 * Tokenè®¡æ•°å·¥å…·
 * ç§»æ¤è‡ªtokencosté¡¹ç›®ï¼Œä»…ä¿ç•™tokenè®¡æ•°åŠŸèƒ½
 */

import { encoding_for_model, get_encoding } from 'tiktoken';
import anthropicTokenizer from '@anthropic-ai/tokenizer';

const { countTokens } = anthropicTokenizer;

/**
 * å»é™¤å¾®è°ƒæ¨¡å‹åç§°å‰ç¼€
 * @param {string} model - æ¨¡å‹åç§°
 * @returns {string} å¤„ç†åçš„æ¨¡å‹åç§°
 */
function stripFtModelName(model) {
  if (model.startsWith('ft:gpt-3.5-turbo')) {
    return 'ft:gpt-3.5-turbo';
  }
  return model;
}

/**
 * è®¡ç®—Anthropic Claudeæ¨¡å‹çš„tokenæ•°é‡
 * @param {Array<{role: string, content: string}>} messages - æ¶ˆæ¯æ•°ç»„
 * @param {string} model - æ¨¡å‹åç§°
 * @returns {number} tokenæ•°é‡
 */
function getAnthropicTokenCount(messages, model) {
  const supportedModels = [
    'claude-opus-4',
    'claude-sonnet-4',
    'claude-3-7-sonnet',
    'claude-3-5-sonnet',
    'claude-3-5-haiku',
    'claude-3-haiku',
    'claude-3-opus',
  ];

  if (!supportedModels.some(supported => model.includes(supported))) {
    throw new Error(
      `${model} is not supported in token counting. Use the usage property in the response for exact counts.`
    );
  }

  try {
    // å°†æ¶ˆæ¯è½¬æ¢ä¸ºæ–‡æœ¬è¿›è¡Œè®¡æ•°
    const text = messages.map(msg => msg.content).join('\n');
    return countTokens(text);
  } catch (error) {
    throw new Error(`Failed to count Anthropic tokens: ${error.message}`);
  }
}

/**
 * è®¡ç®—æ¶ˆæ¯æ•°ç»„çš„tokenæ€»æ•°
 * @param {Array<{role: string, content: string, name?: string}>} messages - æ¶ˆæ¯æ•°ç»„
 * @param {string} model - æ¨¡å‹åç§°
 * @returns {number} tokenæ€»æ•°
 */
export function countMessageTokens(messages, model) {
  let normalizedModel = model.toLowerCase();
  normalizedModel = stripFtModelName(normalizedModel);

  // å¤„ç†Anthropic Claudeæ¨¡å‹
  if (normalizedModel.includes('claude-') && !normalizedModel.startsWith('anthropic.')) {
    console.warn('Warning: Anthropic token counting may have differences!');
    return getAnthropicTokenCount(messages, normalizedModel);
  }

  // ä½¿ç”¨tiktokenå¤„ç†OpenAIæ¨¡å‹
  let encoding;
  try {
    encoding = encoding_for_model(normalizedModel);
  } catch (error) {
    console.warn('Model not found. Using cl100k_base encoding.');
    encoding = get_encoding('cl100k_base');
  }

  let tokensPerMessage = 3;
  let tokensPerName = 1;

  // æ ¹æ®ä¸åŒæ¨¡å‹è®¾ç½®tokenè®¡æ•°è§„åˆ™
  const gpt35TurboModels = new Set([
    'gpt-3.5-turbo-0613',
    'gpt-3.5-turbo-16k-0613',
  ]);

  const gpt4Models = new Set([
    'gpt-4-0314',
    'gpt-4-32k-0314',
    'gpt-4-0613',
    'gpt-4-32k-0613',
    'gpt-4-turbo',
    'gpt-4-turbo-2024-04-09',
    'gpt-4o',
    'gpt-4o-2024-05-13',
  ]);

  if (gpt35TurboModels.has(normalizedModel) || gpt4Models.has(normalizedModel) || normalizedModel.startsWith('o')) {
    tokensPerMessage = 3;
    tokensPerName = 1;
  } else if (normalizedModel === 'gpt-3.5-turbo-0301') {
    tokensPerMessage = 4;
    tokensPerName = -1;
  } else if (normalizedModel.includes('gpt-3.5-turbo')) {
    console.warn('gpt-3.5-turbo may update over time. Using gpt-3.5-turbo-0613 for counting.');
    return countMessageTokens(messages, 'gpt-3.5-turbo-0613');
  } else if (normalizedModel.includes('gpt-4o')) {
    console.warn('gpt-4o may update over time. Using gpt-4o-2024-05-13 for counting.');
    return countMessageTokens(messages, 'gpt-4o-2024-05-13');
  } else if (normalizedModel.includes('gpt-4')) {
    console.warn('gpt-4 may update over time. Using gpt-4-0613 for counting.');
    return countMessageTokens(messages, 'gpt-4-0613');
  } else {
    throw new Error(
      `countMessageTokens() is not implemented for model ${model}. ` +
      'See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.'
    );
  }

  let numTokens = 0;
  for (const message of messages) {
    numTokens += tokensPerMessage;
    for (const [key, value] of Object.entries(message)) {
      if (typeof value === 'string') {
        numTokens += encoding.encode(value).length;
        if (key === 'name') {
          numTokens += tokensPerName;
        }
      }
    }
  }
  numTokens += 3; // æ¯ä¸ªå›å¤éƒ½ä»¥ <|start|>assistant<|message|> å¼€å§‹

  encoding.free();
  return numTokens;
}

/**
 * å°†è‡ªå®šä¹‰æ¨¡å‹åç§°æ˜ å°„åˆ°tiktokenæ”¯æŒçš„æ¨¡å‹
 * @param {string} model - æ¨¡å‹åç§°
 * @returns {string} tiktokenæ”¯æŒçš„æ¨¡å‹åç§°
 */
function mapModelForTiktoken(model) {
  const normalizedModel = model.toLowerCase();
  
  // GPT-OSS å’Œ Gemini æ¨¡å‹ä½¿ç”¨ cl100k_base ç¼–ç ï¼ˆä¸ GPT-4 ç›¸åŒï¼‰
  if (normalizedModel.includes('gpt-oss') ||
      normalizedModel.includes('gemini') ||
      normalizedModel.includes('gpt-4o')) {
    return 'gpt-4o';
  }
  
  // GPT-4 ç³»åˆ—
  if (normalizedModel.includes('gpt-4')) {
    return 'gpt-4';
  }
  
  // GPT-3.5 ç³»åˆ—
  if (normalizedModel.includes('gpt-3.5')) {
    return 'gpt-3.5-turbo';
  }
  
  // o1/o3 ç³»åˆ—æ¨¡å‹
  if (normalizedModel.startsWith('o1') || normalizedModel.startsWith('o3')) {
    return 'gpt-4o';
  }
  
  return model;
}

/**
 * è®¡ç®—å­—ç¬¦ä¸²çš„tokenæ•°é‡
 * @param {string} text - æ–‡æœ¬å­—ç¬¦ä¸²
 * @param {string} model - æ¨¡å‹åç§°
 * @returns {number} tokenæ•°é‡
 */
export function countStringTokens(text, model) {
  let normalizedModel = model.toLowerCase();

  // å¤„ç†å¸¦providerå‰ç¼€çš„æ¨¡å‹å
  if (normalizedModel.includes('/')) {
    normalizedModel = normalizedModel.split('/').pop();
  }

  // Claudeæ¨¡å‹ä½¿ç”¨Anthropic tokenizer
  if (normalizedModel.includes('claude-')) {
    return countTokens(text);
  }

  // æ˜ å°„è‡ªå®šä¹‰æ¨¡å‹åç§°åˆ°tiktokenæ”¯æŒçš„æ¨¡å‹
  const mappedModel = mapModelForTiktoken(normalizedModel);

  let encoding;
  try {
    encoding = encoding_for_model(mappedModel);
  } catch (error) {
    // å¦‚æœæ˜ å°„åçš„æ¨¡å‹ä»ç„¶ä¸æ”¯æŒï¼Œä½¿ç”¨ cl100k_base
    encoding = get_encoding('cl100k_base');
  }

  // ğŸ”¥ ä¿®å¤ï¼šå…è®¸ç‰¹æ®Š tokenï¼Œé¿å… "special token not allowed" é”™è¯¯
  // æ–‡æœ¬ä¸­å¯èƒ½åŒ…å« <|endoftext|> ç­‰ç‰¹æ®Š tokenï¼ˆæ¥è‡ª stopSequences é…ç½®ï¼‰
  // tiktoken é»˜è®¤ä¸å…è®¸è¿™äº›ç‰¹æ®Š tokenï¼Œéœ€è¦æ˜¾å¼è®¾ç½® allowedSpecial
  const tokens = encoding.encode(text, 'all');  // 'all' å…è®¸æ‰€æœ‰ç‰¹æ®Š token
  const count = tokens.length;
  encoding.free();
  
  return count;
}

/**
 * è®¡ç®—promptçš„tokenæ•°é‡
 * @param {Array<{role: string, content: string}>|string} prompt - æ¶ˆæ¯æ•°ç»„æˆ–å­—ç¬¦ä¸²
 * @param {string} model - æ¨¡å‹åç§°
 * @returns {number} tokenæ•°é‡
 */
export function countPromptTokens(prompt, model) {
  const normalizedModel = model.toLowerCase();
  const strippedModel = stripFtModelName(normalizedModel);

  if (!Array.isArray(prompt) && typeof prompt !== 'string') {
    throw new TypeError(
      `Prompt must be either a string or array of message objects but found ${typeof prompt} instead.`
    );
  }

  if (typeof prompt === 'string' && !strippedModel.includes('claude-')) {
    return countStringTokens(prompt, model);
  } else {
    return countMessageTokens(prompt, model);
  }
}

/**
 * è®¡ç®—completionçš„tokenæ•°é‡
 * @param {string} completion - å®Œæˆæ–‡æœ¬
 * @param {string} model - æ¨¡å‹åç§°
 * @returns {number} tokenæ•°é‡
 */
export function countCompletionTokens(completion, model) {
  const strippedModel = stripFtModelName(model);

  if (typeof completion !== 'string') {
    throw new TypeError(
      `Completion must be a string but found ${typeof completion} instead.`
    );
  }

  if (strippedModel.includes('claude-')) {
    const completionList = [{ role: 'assistant', content: completion }];
    // Anthropicåœ¨å®é™…completion tokensä¸Šé™„åŠ çº¦13ä¸ªé¢å¤–tokens
    return countMessageTokens(completionList, model) - 13;
  } else {
    return countStringTokens(completion, model);
  }
}

/**
 * è®¡ç®—æ‰€æœ‰tokensï¼ˆprompt + completionï¼‰
 * @param {Array<{role: string, content: string}>|string} prompt - æ¶ˆæ¯æ•°ç»„æˆ–å­—ç¬¦ä¸²
 * @param {string} completion - å®Œæˆæ–‡æœ¬
 * @param {string} model - æ¨¡å‹åç§°
 * @returns {{promptTokens: number, completionTokens: number, totalTokens: number}}
 */
export function countAllTokens(prompt, completion, model) {
  const promptTokens = countPromptTokens(prompt, model);
  const completionTokens = countCompletionTokens(completion, model);

  return {
    promptTokens,
    completionTokens,
    totalTokens: promptTokens + completionTokens,
  };
}

export default {
  countMessageTokens,
  countStringTokens,
  countPromptTokens,
  countCompletionTokens,
  countAllTokens,
};